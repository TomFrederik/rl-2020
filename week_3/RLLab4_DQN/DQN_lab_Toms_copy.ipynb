{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Deep Q Network\n",
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dqn_autograde.py file into codegrade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custommagics import CustomMagics\n",
    "get_ipython().register_magics(CustomMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile dqn_autograde.py\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "def tqdm(*args, **kwargs):\n",
    "    return _tqdm(*args, **kwargs, mininterval=1)  # Safety, do not overflow buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc69f22067705372",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fef7e20e54e6243b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 1. Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-39519f4ab05eb2a1",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erik/.miniconda3/envs/rl2020/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.envs.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        CartPoleEnv\n",
       "\u001b[0;31mString form:\u001b[0m <CartPoleEnv<CartPole-v1>>\n",
       "\u001b[0;31mFile:\u001b[0m        ~/.miniconda3/envs/rl2020/lib/python3.7/site-packages/gym/envs/classic_control/cartpole.py\n",
       "\u001b[0;31mSource:\u001b[0m     \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mCartPoleEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Description:\u001b[0m\n",
       "\u001b[0;34m        A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Source:\u001b[0m\n",
       "\u001b[0;34m        This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Observation: \u001b[0m\n",
       "\u001b[0;34m        Type: Box(4)\u001b[0m\n",
       "\u001b[0;34m        Num     Observation                 Min         Max\u001b[0m\n",
       "\u001b[0;34m        0       Cart Position             -4.8            4.8\u001b[0m\n",
       "\u001b[0;34m        1       Cart Velocity             -Inf            Inf\u001b[0m\n",
       "\u001b[0;34m        2       Pole Angle                 -24°           24°\u001b[0m\n",
       "\u001b[0;34m        3       Pole Velocity At Tip      -Inf            Inf\u001b[0m\n",
       "\u001b[0;34m        \u001b[0m\n",
       "\u001b[0;34m    Actions:\u001b[0m\n",
       "\u001b[0;34m        Type: Discrete(2)\u001b[0m\n",
       "\u001b[0;34m        Num     Action\u001b[0m\n",
       "\u001b[0;34m        0       Push cart to the left\u001b[0m\n",
       "\u001b[0;34m        1       Push cart to the right\u001b[0m\n",
       "\u001b[0;34m        \u001b[0m\n",
       "\u001b[0;34m        Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Reward:\u001b[0m\n",
       "\u001b[0;34m        Reward is 1 for every step taken, including the termination step\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Starting State:\u001b[0m\n",
       "\u001b[0;34m        All observations are assigned a uniform random value between ±0.05\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Episode Termination:\u001b[0m\n",
       "\u001b[0;34m        Pole Angle is more than ±12°\u001b[0m\n",
       "\u001b[0;34m        Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\u001b[0m\n",
       "\u001b[0;34m        Episode length is greater than 200\u001b[0m\n",
       "\u001b[0;34m        Solved Requirements\u001b[0m\n",
       "\u001b[0;34m        Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m'render.modes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m'video.frames_per_second'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgravity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9.8\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasscart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasspole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_mass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasspole\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasscart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;31m# actually half the pole's length\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolemass_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasspole\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce_mag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.02\u001b[0m  \u001b[0;31m# seconds between state updates\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkinematics_integrator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'euler'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Angle at which to fail the episode\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_threshold_radians\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m360\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.4\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mhigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_threshold\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_threshold_radians\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_beyond_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseeding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%r (%s) invalid\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mforce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce_mag\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce_mag\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcostheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msintheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolemass_length\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msintheta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_mass\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mthetaacc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgravity\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msintheta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcostheta\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasspole\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcostheta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcostheta\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_mass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mxacc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolemass_length\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mthetaacc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcostheta\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_mass\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkinematics_integrator\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'euler'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mx\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mx_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_dot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxacc\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtheta_dot\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtheta_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mthetaacc\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# semi-implicit euler\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mx_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_dot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxacc\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mx\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtheta_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mthetaacc\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtheta_dot\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_dot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta_dot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_threshold\u001b[0m \\\n",
       "                \u001b[0;32mor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_threshold\u001b[0m \\\n",
       "                \u001b[0;32mor\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_threshold_radians\u001b[0m \\\n",
       "                \u001b[0;32mor\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_threshold_radians\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_beyond_done\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# Pole just fell!\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_beyond_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_beyond_done\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_beyond_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_beyond_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mscreen_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mscreen_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mworld_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_threshold\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscreen_width\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mworld_width\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcarty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;31m# TOP OF CART\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mpolewidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mpolelen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcartwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcartheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0maxleoffset\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mcart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFilledPolygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcarttrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mcart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcarttrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_geom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mpolewidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolewidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolelen\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpolewidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpolewidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFilledPolygon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxleoffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpole\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcarttrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_geom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_circle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolewidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcarttrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_geom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcarty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcarty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_geom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcartx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;31m# MIDDLE OF CART\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcarttrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcartx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcarty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# env is a TimeLimit wrapper around an env, so use env.env to look into the env (but otherwise you can forget about this)\n",
    "??env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nice thing about the CARTPOLE is that it has very nice rendering functionality (if you are on a local environment). Let's have a look at an episode\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    obs, reward, done, _ = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    time.sleep(0.05)\n",
    "env.close()  # Close the environment or you will have a lot of render screens soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d83f70e62b99520",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Remember from the previous lab, that in order to optimize a policy we need to estimate the Q-values (e.g. estimate the *action* values). In the CartPole problem, our state is current position of the cart, the current velocity of the cart, the current (angular) position of the pole and the (angular) speed of the pole. As these are continuous variables, we have an infinite number of states (ignoring the fact that a digital computer can only represent finitely many states in finite memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0b3162496f5e6cf5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Implement Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-96a86bcfa1ebc84a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will not use the tabular approach but approximate the Q-value function by a general approximator function. We will skip the linear case and directly use a two layer Neural Network. We use [PyTorch](https://pytorch.org/) to implement the network, as this will allow us to train it easily later. We can implement a model using `torch.nn.Sequential`, but with PyTorch it is actually very easy to implement the model (e.g. the forward pass) from scratch. Now implement the `QNetwork.forward` function that uses one hidden layer with ReLU activation (no output activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-216429a5dccf8a0e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden=128):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, num_hidden)\n",
    "        self.l2 = nn.Linear(num_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## My Code ##\n",
    "        out = self.l1(x)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.l2(out)\n",
    "        \n",
    "        return out\n",
    "        ## ##\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-00ce108d640a5942",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's instantiate and test if it works\n",
    "num_hidden = 128\n",
    "torch.manual_seed(1)\n",
    "Q_net = QNetwork(num_hidden)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "test_model = nn.Sequential(\n",
    "    nn.Linear(4, num_hidden), \n",
    "    nn.ReLU(), \n",
    "    nn.Linear(num_hidden, 2)\n",
    ")\n",
    "\n",
    "x = torch.rand(10, 4)\n",
    "\n",
    "# If you do not need backpropagation, wrap the computation in the torch.no_grad() context\n",
    "# This saves time and memory, and PyTorch complaints when converting to numpy\n",
    "with torch.no_grad():\n",
    "    assert np.allclose(Q_net(x).numpy(), test_model(x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca77eae2e62180cf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2c1d117a1a75fd69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to stabilize learning, we will use an experience replay to save states in and sample states from. Now implement the `push` function that adds a transition to the replay buffer, and the `sample` function that samples a (random!) batch of data, for use during training (hint: you can use the function `random.sample`). It should keep at most the maximum number of transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3cc876e51eb157f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        ## MY CODE ##\n",
    "        \n",
    "        # if at full capacity, remove oldest transition\n",
    "        if self.capacity == self.__len__():\n",
    "            del self.memory[0]\n",
    "            \n",
    "        # add new transition\n",
    "        self.memory.append(transition)\n",
    "        \n",
    "        ## ##\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ## MY CODE ##\n",
    "        \n",
    "        # get a sample from the memory\n",
    "        sample = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        return sample\n",
    "        ## ##\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3b90135921c4da76",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([ 0.03171285,  0.00969365,  0.0325802 , -0.02156947]), 0, 1.0, array([ 0.03190672, -0.18588002,  0.03214881,  0.28121224]), False)]\n"
     ]
    }
   ],
   "source": [
    "capacity = 10\n",
    "memory = ReplayMemory(capacity)\n",
    "\n",
    "# Sample a transition\n",
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "s_next, r, done, _ = env.step(a)\n",
    "\n",
    "# Push a transition\n",
    "memory.push((s, a, r, s_next, done))\n",
    "\n",
    "# Sample a batch size of 1\n",
    "print(memory.sample(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88f67e3c051da6a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3 $\\epsilon$psilon greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa3c7d1b3000f697",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to learn a good policy, we need to explore quite a bit initially. As we start to learn a good policy, we want to decrease the exploration. As the amount of exploration using an $\\epsilon$-greedy policy is controlled by $\\epsilon$, we can define an 'exploration scheme' by writing $\\epsilon$ as a function of time. There are many possible schemes, but we will use a simple one: we will start with only exploring (so taking random actions) at iteration 0, and then in 1000 iterations linearly anneal $\\epsilon$ such that after 1000 iterations we take random (exploration) actions with 5\\% probability (forever, as you never know if the environment will change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5789e7a792108576",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "def get_epsilon(it):\n",
    "    #$ MY CODE ##\n",
    "    epsilon = max(0.05, 1 - (0.95/1000 * it))\n",
    "    \n",
    "    ## ##\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-40e66db45e742b2e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe6d8233be0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# So what's an easy way to check?\n",
    "plt.plot([get_epsilon(it) for it in range(5000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8b604c9998c6c3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now write a function of *EpsilonGreedyPolicy* class. This function takes a state and uses the Q-network to select an ($\\epsilon$-greedy) action. It should return a random action with probability epsilon. Note, you do not need to backpropagate through the model computations, so use `with torch.no_grad():` (see above for example). Note that to convert a PyTorch tensor with only 1 element (0 dimensional) to a simple python scalar (int or float), you can use the '.item()' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-878ad3a637cfb51c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "class EpsilonGreedyPolicy(object):\n",
    "    \"\"\"\n",
    "    A simple epsilon greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(self, Q, epsilon):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def sample_action(self, obs):\n",
    "        \"\"\"\n",
    "        This method takes a state as input and returns an action sampled from this policy.  \n",
    "\n",
    "        Args:\n",
    "            obs: current state\n",
    "\n",
    "        Returns:\n",
    "            An action (int).\n",
    "        \"\"\"\n",
    "        ## MY CODE ##\n",
    "        \n",
    "        if random.random() <= self.epsilon:\n",
    "            action = random.randint(0,1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                Q_values = self.Q(torch.from_numpy(obs).float())\n",
    "                action = torch.argmax(Q_values).item()\n",
    "        \n",
    "        return action\n",
    "        ## ##\n",
    "        \n",
    "        \n",
    "        \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.epsilon = epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e895338d56bee477",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "epg = EpsilonGreedyPolicy(Q_net, 0.05)\n",
    "a = epg.sample_action(s)\n",
    "assert not torch.is_tensor(a)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec5e94e0b03f8aec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.4 Training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1a12cc97386fe56",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will implement the function 'train' that samples a batch from the memory and performs a gradient step using some convenient PyTorch functionality. However, you still need to compute the Q-values for the (state, action) pairs in the experience, as well as their target (e.g. the value they should move towards). What is the target for a Q-learning update? What should be the target if `next_state` is terminal (e.g. `done`)?\n",
    "\n",
    "For computing the Q-values for the actions, note that the model returns all action values where you are only interested in a single action value. Because of the batch dimension, you can't use simple indexing, but you may want to have a look at [torch.gather](https://pytorch.org/docs/stable/torch.html?highlight=gather#torch.gather) or use [advanced indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html) (numpy tutorial but works mostly the same in PyTorch). Note, you should NOT modify the function train. You can view the size of a tensor `x` with `x.size()` (similar to `x.shape` in numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c45485324b40081",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "def compute_q_vals(Q, states, actions):\n",
    "    \"\"\"\n",
    "    This method returns Q values for given state action pairs.\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-net\n",
    "        states: a tensor of states. Shape: batch_size x obs_dim\n",
    "        actions: a tensor of actions. Shape: Shape: batch_size x 1\n",
    "\n",
    "    Returns:\n",
    "        A torch tensor filled with Q values. Shape: batch_size x 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## MY CODE ##\n",
    "    \n",
    "    # perform a forward pass\n",
    "    all_qs = Q(states) # should have shape batch_size x |A|\n",
    "    #print('all_qs.shape = ', all_qs.shape)\n",
    "    \n",
    "    # get q values of actions that were actually taken\n",
    "    q_vals = all_qs[np.arange(all_qs.shape[0])[:, None], actions] # should have shape batch_size x 1\n",
    "    #print('q_vals.shape = ', q_vals.shape)\n",
    "    \n",
    "    return q_vals\n",
    "    ## ##\n",
    "    \n",
    "def compute_targets(Q, rewards, next_states, dones, discount_factor):\n",
    "    \"\"\"\n",
    "    This method returns targets (values towards which Q-values should move).\n",
    "    \n",
    "    Args:\n",
    "        Q: Q-net\n",
    "        rewards: a tensor of actions. Shape: Shape: batch_size x 1\n",
    "        next_states: a tensor of states. Shape: batch_size x obs_dim\n",
    "        dones: a tensor of boolean done flags (indicates if next_state is terminal) Shape: batch_size x 1\n",
    "        discount_factor: discount\n",
    "    Returns:\n",
    "        A torch tensor filled with target values. Shape: batch_size x 1.\n",
    "    \"\"\"\n",
    "    ## MY CODE ##\n",
    "    \n",
    "    # compute all q_vals\n",
    "    all_qs = Q(next_states)\n",
    "    #print('all_qs.shape = ', all_qs.shape) # should have shape batch_size x |A|\n",
    "    \n",
    "    # if a state was done, then the q-values should be set to zero:\n",
    "    all_qs = ~dones * all_qs\n",
    "    #print('masked all_qs.shape = ', all_qs.shape) # should have shape batch_size x |A|\n",
    "    \n",
    "    # get the maximum q_value for each next state\n",
    "    max_qs = torch.max(all_qs, dim=1)[0] # only want value, not action\n",
    "    #print('max_qs.shape = ', max_qs.shape) # is of shape [64]\n",
    "    \n",
    "    # compute targets\n",
    "    targets = rewards[:,0] + discount_factor * max_qs\n",
    "    targets = targets.unsqueeze(1)\n",
    "    \n",
    "    return targets\n",
    "    ## ##\n",
    "\n",
    "def train(Q, memory, optimizer, batch_size, discount_factor):\n",
    "    # DO NOT MODIFY THIS FUNCTION\n",
    "    \n",
    "    # don't learn without some decent experience\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    # transition is a list of 4-tuples, instead we want 4 vectors (as torch.Tensor's)\n",
    "    state, action, reward, next_state, done = zip(*transitions)\n",
    "    \n",
    "    # convert to PyTorch and define types\n",
    "    state = torch.tensor(state, dtype=torch.float)\n",
    "    action = torch.tensor(action, dtype=torch.int64)[:, None]  # Need 64 bit to use them as index\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "    reward = torch.tensor(reward, dtype=torch.float)[:, None]\n",
    "    done = torch.tensor(done, dtype=torch.uint8)[:, None]  # Boolean\n",
    "    \n",
    "    # compute the q value\n",
    "    q_val = compute_q_vals(Q, state, action)\n",
    "    with torch.no_grad():  # Don't compute gradient info for the target (semi-gradient)\n",
    "        target = compute_targets(Q, reward, next_state, done, discount_factor)\n",
    "    \n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "\n",
    "    # backpropagation of loss to Neural Network (PyTorch magic)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()  # Returns a Python scalar, and releases history (similar to .detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b060b822eec4282f",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.267250061035156\n"
     ]
    }
   ],
   "source": [
    "# You may want to test your functions individually, but after you do so lets see if the method train works.\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "# Simple gradient descent may take long, so we will use Adam\n",
    "optimizer = optim.Adam(Q_net.parameters(), learn_rate)\n",
    "\n",
    "# We need a larger memory, fill with dummy data\n",
    "transition = memory.sample(1)[0]\n",
    "memory = ReplayMemory(10 * batch_size)\n",
    "for i in range(batch_size):\n",
    "    memory.push(transition)\n",
    "\n",
    "# Now let's see if it works\n",
    "loss = train(Q_net, memory, optimizer, batch_size, discount_factor)\n",
    "\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3eafd0ab49103f3b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.5 Put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-36b8a04b393d8104",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now that you have implemented the training step, you should be able to put everything together. Implement the function `run_episodes` that runs a number of episodes of DQN training. It should return the durations (e.g. number of steps) of each episode. Note: we pass the train function as an argument such that we can swap it for a different training step later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-540a7d50ecc1d046",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dqn_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dqn_autograde.py\n",
    "\n",
    "def run_episodes(train, Q, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate):\n",
    "    \n",
    "    optimizer = optim.Adam(Q.parameters(), learn_rate)\n",
    "    \n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        steps = 0\n",
    "        while True:\n",
    "            ## MY CODE ##\n",
    "            \n",
    "            # get current epsilon\n",
    "            eps = get_epsilon(global_steps)\n",
    "            \n",
    "            # set eps for policy\n",
    "            policy.set_epsilon(eps)\n",
    "            \n",
    "            # get new action\n",
    "            action = policy.sample_action(state)\n",
    "            \n",
    "            # get new transition\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            #env.render()\n",
    "            #time.sleep(0.05)\n",
    "\n",
    "            # store transition in memory\n",
    "            memory.push((state, action, reward, next_state, done))\n",
    "            \n",
    "            # set s <= s'\n",
    "            state = next_state\n",
    "            \n",
    "            # sample a batch from memory\n",
    "            batch = memory.sample(min(batch_size, len(memory)))\n",
    "                        \n",
    "            # compute batch loss\n",
    "            loss = train(Q, memory, optimizer, batch_size, discount_factor)\n",
    "            \n",
    "            # inc step counter\n",
    "            steps += 1\n",
    "            global_steps += 1\n",
    "            \n",
    "            ## ##\n",
    "            \n",
    "            if done:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                          .format(i, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "                episode_durations.append(steps)\n",
    "                #plot_durations()\n",
    "                break\n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 0 finished after 11 steps\n",
      "\u001b[99m Episode 10 finished after 24 steps\n",
      "\u001b[99m Episode 20 finished after 14 steps\n",
      "\u001b[99m Episode 30 finished after 10 steps\n",
      "\u001b[99m Episode 40 finished after 10 steps\n",
      "\u001b[99m Episode 50 finished after 9 steps\n",
      "\u001b[99m Episode 60 finished after 9 steps\n",
      "\u001b[99m Episode 70 finished after 8 steps\n",
      "\u001b[99m Episode 80 finished after 11 steps\n",
      "\u001b[99m Episode 90 finished after 9 steps\n"
     ]
    }
   ],
   "source": [
    "# Let's run it!\n",
    "num_episodes = 100\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "memory = ReplayMemory(10000)\n",
    "num_hidden = 128\n",
    "seed = 42  # This is not randomly chosen\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "Q_net = QNetwork(num_hidden)\n",
    "policy = EpsilonGreedyPolicy(Q_net, 0.05)\n",
    "episode_durations = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-928ecc11ed5c43d8",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Episode durations per episode')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd3gc1dX48e/ZVe9Wtaqb3ORuC9vYgDE9FBsSCKEaQi8J4UeAUEIqeQkkIbxpYEpIsOnNwEsNATewbLn3Jtuyiq1mdavf3x87sleyZNXVarXn8zx6tDszO3NmdnV0984tYoxBKaWU57G5OwCllFLdowlcKaU8lCZwpZTyUJrAlVLKQ2kCV0opD6UJXCmlPJQm8AFMRD4RkQW9vM9fisiiXtrXyyLy297YVyePd42IfN5Xx+vvRKRSRIb38j6/FpGbe3Ofqn0+7g5AnZyI7AfigEanxS8bY+7u6LXGmO+4Kq7+TkSGAvsAX2NMA4AxZjGw2I1h9SvGmBB3x6B6RhO4Z7jEGPMfdwfRn4iI3RjT2PGWA4OI+DT/I1KqmVaheDARuUFEVorIX0SkTER2iMjZTuuPfZ0VkVQRWWptVyQibzhtN0tE1ljr1ojILKd1w6zXVYjIF0B0qxhmisg3IlIqIhtF5MyTxDtFRNZZ+3oDCGh1LitabW9EJNV6/LKI/ENEPhaRKmCuiFwkIutFpFxEDorIL51evsz6XWpVFZza+hgdnPfXIvIb6/pWiMjnIhJtrQsQkUUiUmyd9xoRiWvnnPeLyEMisk1EjojIP0XE+bwvFpEN1n6+EZGJrV77oIhsAqpE5IQCl4iMEZEvRKRERHaKyPed1r0sIs9a6yus93FIO9f3QivGChHJFZGfOm13i4jssY7xgYgkOK071/rclYnIXwFpFd8PRWS7de6fOR9f9QJjjP704x9gP3BOO+tuABqAewFf4EqgDIi01n8N3Gw9fg14BMc/7QDgNGt5JHAEuA7HN7KrrOdR1vpvgT8B/sAZQAWwyFqXCBQDF1r7Pdd6HtNGrH7AAadYLwfqgd86ncuKVq8xQKr1+GXr3GY7ncOZwATr+UTgMHCptf1Q6/U+ra7Xik6e99fAXmAUEGg9f8JadxvwIRAE2IFpQNhJ3r8tQLJ1zJVO5zwVKABmWPtZYG3v7/TaDdZrA9vYdzBwELjROoepQBEwzumaVVjvmz/wjPM1bnV984HTrceDgKnW47OsfU619vEXYJm1Lhoot95LX+u9beD4Z+5SYA8w1orvUeAbd/9NDaQftwegPx28QY4/4kqg1OnnFmvdDUAeIE7brwausx5/7fTH9G9gIZDUav/XAatbLfvW2neK9QcZ7LTuVY4n8AeBV1q99jNgQRvncUYbsX5D1xL4vzu4Vn8GnrYeD+XkCbzd83a6do86rbsT+NR6/EMr9omdfP9ud3p+IbDXevwP4Dettt8JzHF67Q9Psu8rgeWtlj0H/MLpmr3utC4Ex72U5DaubzaOf0xhrfb3IvBkq33UW9f3emCV0zoBcpw+c58ANzmttwHVwBB3/10NlB+tQvEMlxpjIpx+nndal2usvw7LASCBEz2A4w9stYhsFZEfWssTrNc4O4CjdJ0AHDHGVLVa12wIcIX19b9UREqB04D4No6f0E6sXXHQ+YmIzBCRr0SkUETKgNtpVcVzEic772aHnB5X40heAK/g+Ef1uojkiciTIuLbybid358hwH2trl8yLd+/FufcyhBgRqvXXwMMbuv1xphKoIS2Px/fw/HP5YBV1XKqtbzFdbL2Uczxz4fz/k2reIcAzzjFVoLjM+h8jVUPaAL3fIki4lzvmIKjpNuCMeaQMeYWY0wCjpLW3636zzwcf2i02kcujq/Vg0QkuNW6ZgdxlMCd/7kEG2OeaCPO/HZibVaFo0oCABFxTkLHTqPV81eBD3CUKMOBZzleB9vRMJsnO++TMsbUG2N+ZYxJA2YBF+MojbYnudUxmt+fg8Djra5fkDHmNefDnWS/B4GlrV4fYoy5o61ji0gIjmqctj4fa4wx84FY4H3gTWtVi+tkfRaiOP75cN6/tDrXg8BtreILNMZ8c5JzUl2gCdzzxQI/FhFfEbkCR33jx603EpErRCTJenoER2JotLYdJSJXi4iPiFwJpAEfGWMOAJnAr0TET0ROAy5x2u0i4BIROV9E7NbNvTOdjuPsWxzVMT+2jvNdYLrT+o3AOBGZbN3k+2Unzj0UKDHG1IjIdOBqp3WFQBPQXjvnds+7o4OKyFwRmSAidhx1wPW0bObZ2l0ikiQikcDDQPMN5OeB261vEiIiweK4MRvaUQyWj6xzuM56/31F5BQRGeu0zYUicpqI+AG/ATKMMa2/yfiJo418uDGm3jqn5vN5FbjRel/8gd9Z+9gP/B+O9+y71g3WH9Oy9P8s8JCIjLOOE259RlUv0QTuGT4UR0uK5p/3nNZlACNx3Gh6HLjcGFPcxj5OATJEpBJHqfUeY8w+a9uLgftwfDV+ALjYGFNkve5qHDfZSoBf4KhLB8BKBPNxJKVCHCWu+2njc2WMqQO+i6Me+giO+tt3ndbvAn4N/AfYDaxovY823An8WkQqgMc4XmrEGFNtXY+V1lf4ma3i6ei8T2Yw8DaORLcdWIrjn1l7XgU+B7Ksn99aMWQCtwB/xXFN9uC4Pp1ijKkAzgN+gKOkfAj4PY6bjc7H/gWO928ajiqWtlwH7BeRchxVUddax/gS+DnwDo4S9wjreFjX6grgCRzXcCSOm7TN8b1nxfO6td8tgNf2TXAFaVklqTyJiNyA44bRae6ORbVNHB2xbjZuaMcvIi8DOcaYR/v62KpvaAlcKaU8lCZwpZTyUFqFopRSHqrDEriIJFttbbdb7YfvabX+p+LoktvZ9rdKKaV6QWcGs2oA7jPGrLOaN60VkS+MMdtEJBlH9+nszhwsOjraDB06tPvRKqWUF1q7dm2RMSam9fIOE7gxJh9H8yGMMRUish1HT6ptwNM4ml8t6UwQQ4cOJTMzsytxK6WU1xORNnstd+kmpjjGWJ6Coz3xPBxdozd28JpbRSRTRDILCwu7cjillFIn0ekEbnXDfQf4CY5qlUdwdJ44KWPMQmNMujEmPSbmhG8ASimluqlTCdwaqOcdYLEx5l0cvbGGARutjgpJwLp2xq9QSinlAh3WgVsD1LwIbDfG/AnAGLMZxxgczdvsB9I72Q1ZKaVUL+hMCXw2jnESzhLHzCEbRORCF8ellFKqA51phbKCVtMktbHN0N4KSCmlVOdoV3qllPJQHpfAG5sMr63O5mid10xIrpRSbfK4BL4qq5iH3t3Ma6s71flTKaUGLI9L4FvzygBYsvGEWaGUUsqreGACLwdg48FS9hdVdbC1UkoNXB6XwLfllTM+MQwR+EBL4UopL+ZRCfxoXSN7Cys5a3QsM4ZF8v6GXHQ8c6WUt/KoBL7zcAVNBtISwpk/OZGswqpjVSpKKeVtPCqBN9/AHJcQxnfGD8bXLizZkOvmqJRSyj08KoFvyysnNMCHpEGBRAT5MWdULB9szKOxSatRlFLex6MS+Na8ctLiw3CMrwXzJydwuLyW1ftK3ByZUkr1PY9J4I1Nhh2HyhmXEH5s2Tlj4wjys2trFKWUV/KYBL6vqJKa+ibSEsKOLQv0szM7NZqVe3QUW6WU9/GYBN7c2mScUwIHmDk8iuySavLLjrojLKWUchuPSeDb8srxs9tIjQ1psXzGsEgAMrK0Hlwp5V08J4HnlzNqcAi+9pYhj40PIzTAh4x9xW6KTCml3MMjErgxhq155YyLDz9hnd0mTB8aySotgSulvIxHJPDD5bWUVNW1uIHpbMbwSPYVVVFQXtPHkSmllPt0mMBFJFlEvhKR7SKyVUTusZY/JSI7RGSTiLwnIhGuCtK5B2ZbZgyLAmCVtgdXSnmRzpTAG4D7jDFjgZnAXSKSBnwBjDfGTAR2AQ+5KshteeWIwJj4thP4uIQwQvx9yMjSenCllPfoMIEbY/KNMeusxxXAdiDRGPO5MabB2mwVkOSqIJsMTEmOIMS/7TmYfew2pg0ZRIaWwJVSXqRLdeAiMhSYAmS0WvVD4JN2XnOriGSKSGZhYWF3YuSec0by7p2zT7rNjOGR7CmopKiytlvHUEopT9PpBC4iIcA7wE+MMeVOyx/BUc2yuK3XGWMWGmPSjTHpMTExPY23Xc314DouilLKW3QqgYuIL47kvdgY867T8gXAxcA1xs0zK0xMCifQ16714Eopr9F2pbITcQz99yKw3RjzJ6flFwAPAnOMMdWuC7FzfO020odqPbhSynt0pgQ+G7gOOEtENlg/FwJ/BUKBL6xlz7oy0M6YOTyKHYcqyC52+/8TpZRyuc60QllhjBFjzERjzGTr52NjTKoxJtlp2e19EfDJXD4tCT8fG/9YusfdoSillMt5RE/MzooLC+AHpyTz9tocco5oKVwpNbANqAQOcPucEQA8u3SvmyNRSinXGnAJPCEikCvSk3lzTY6OEa6UGtAGXAIHuGPOCJqM4bmlWe4ORSmlXGZAJvDkyCAun5bEq6uzdYRCpdSANSATOMCdZ6ZS19DEO+ty3R2KUkq5xIBN4ClRQQyNCmLjwVJ3h6KUUi4xYBM4wISkCDblaAJXSg1MAzqBT0oKJ6+shsIKHaFQKTXwDOgEPjHJMUnQ5lwthSulBp4BncDHJYRhE9iUU+buUJRSqtcN6AQe7O9DamyIJnCl1IA0oBM4wIREx41MNw9XrpRSvW7AJ/BJyeEUVdaRX6YdepRSA8uAT+ATEsMBWjQnLCiv4ZH3NlNd19Dey5RSqt8b8Al8bHwYPjZpUQ/+9H92sTgjm/XZ2jpFKeW5BnwCD/C1MyY+9FgCzzlSzdtrcwDIK9XRCpVSnmvAJ3BoeSPTeZzwQ1ovrpTyYF6RwCclhVNe00DGvhLeXJPD5dOSiQ7xI08TuFLKg3WYwEUkWUS+EpHtIrJVRO6xlkeKyBcistv6Pcj14XbPhCTHjcz7395IkzHceeYIBocH6IQPSimP1pkSeANwnzFmLDATuEtE0oCfAV8aY0YCX1rP+6VRcaH4+9g4WHKU701NIjkyiPjwQK1CUUp5tM7MSp9vjFlnPa4AtgOJwHzgX9Zm/wIudVWQPeVrt5GWEIbdJtw51zFnZnx4gN7EVEp5NJ+ubCwiQ4EpQAYQZ4zJB0eSF5HYdl5zK3ArQEpKSk9i7ZEfnzWS/LIahkQFAxAfHkh5TQNVtQ0E+3fpMiilVL/Q6cwlIiHAO8BPjDHlItKp1xljFgILAdLT093Wn33umJb/XxIiAgDIL6shNTbEHSEppVSPdKoVioj44kjei40x71qLD4tIvLU+HihwTYiuMTisOYFrNYpSyjN1phWKAC8C240xf3Ja9QGwwHq8AFjS++G5TkJEIAD5pXojUynlmTpThTIbuA7YLCIbrGUPA08Ab4rITUA2cIVrQnSNuLDjVShKKeWJOkzgxpgVQHsV3mf3bjh9x8/HRnSIv1ahKKU8llf0xGxPQkSA9sZUSnksr07g8eEBHNISuFLKQ3l5Ag/Um5hKKY/l5Qk8gIraBipq6t0dilJKdZl3J3CrKaGOiaKU8kTencDDHU0J9UamUsoTaQIH8nVQK6WUB/LqBB4XFoCIduZRSnkmr07gvnYbMdqZRynlobw6gYPjRqaWwJVSnkgTeFiAJnCllEfSBB4RQH7pUYxx21DlSinVLV6fwBPCA6mqa6S8psHdoSilVJd4fQIfbDUl1M48SilP4/UJvHlqtTxtiaKU8jBen8Djw3VmHqWUZ/L6BB4b6o9N0GFllVIex+sTuI/dxoiYEL7aWagtUZRSHqUzkxq/JCIFIrLFadlkEVklIhtEJFNEprs2TNe65fThbM4t4+udhe4ORSmlOq0zJfCXgQtaLXsS+JUxZjLwmPXcY102NZHEiECe+XK3lsKVUh6jwwRujFkGlLReDIRZj8OBvF6Oq0/52m3cNTeVDQdLWb67yN3hKKVUp3S3DvwnwFMichD4A/BQexuKyK1WNUtmYWH/raK4fFoSCeEBWgpXSnmM7ibwO4B7jTHJwL3Ai+1taIxZaIxJN8akx8TEdPNwrufnY+OOuamsPXCEb/cWuzscpZTqUHcT+ALgXevxW4BH38Rs9v30JAaHOUrhSinV33U3gecBc6zHZwEDIuP5+9i58pRkMvaVUFWrY6Mopfo3n442EJHXgDOBaBHJAX4B3AI8IyI+QA1wqyuD7EtjBocCsK+oivGJ4W6ORiml2tdhAjfGXNXOqmm9HEu/MDwmBIC9hZWawJVS/ZrX98RsbUhUEDaBvYVVfXrc3NKj/HPlPoora/v0uEopz9VhCdzbBPjaSRoURFZhpcuPZYzhq50FLF6VzVc7C2gykFd6lEcuSnP5sZVSnk9L4G0YHhNMVh+UwN/KzOGHL2eyKbeMu+amcurwKD7YmEdjk7ZDV0p1TBN4G0bEhJBVVEmTixPpptxSwgN9+eZnZ3HfeaO5akYKh8trWb2vdcdXpZQ6kSbwNgyPCaamvon8cteOEZ5XWkNiRCC+dsfbcM7YWIL87HywMdelx1VKDQyawNsworklSoFr68HzSo+SEBF47HmQnw/npcXx8eZD1DY0uvTYSinPpwm8DcNjggFcfiMzt/QoidaUbs3mT0mk7Gg9y3bpoFpKqZPTBN6GmBB/Qv19yCpy3Y3M8pp6KmoaWpTAAU5LjSYy2I8lG45Xo+w8VMHj/7eNpbsKXV4vr5TyHNqMsA0iwvDYEPa6sATePAdn6wTua7dx0YR43lp7kMraBlbsLuT/vbmR6rpGnl++j5TIIK6ekcK1M4cQ4q9vn1LeTEvg7RgR7dqmhHmljjk4WydwgPmTE6ipb+K2VzK5fdE6RsaFsvyBufzvVVOIDw/giU928OPX1uuwt0p5OU3g7RgRG0J+WY3LBrXKtRJ4YhsJfGrKIBIjAlm5p5jvTU3ijVtnkhwZxLxJCbxx26k8fOEY/rujgM+2HnZJbEopz6DfwdsxPNpxI9NVg1rllR7F1y7EhvqfsM5mE568fCIFFTVcOjkREWmx/oezh/Huulx+9eFWTh8ZTbBWpSjllbQE3g7nQa1cIa/0KIPDA7DZpM31s1OjuWxK0gnJG8DHbuPxyyaQX1bDn/+zyyXxKaX6P03g7XD1oFZ5pTUkhJ9YfdJZ04YM4qrpKby0cj/b8sp7MTKllKfQBN4OVw9q5WgD3v0EDvDgBaOJCPTlx6+v59Mth2hobOql6JRSnkAT+EmMiAl2SQm8sclwqLymzRYoXRER5McfrphEVW0Dty9ay+zf/5env9hFTb324lTKG2gCP4nhMSHsc8GgVgUVNTQ2mR4ncIC5Y2JZ/sBcnr8+nbHxYTzz5W7+8NnOXohSKdXfaQI/CVcNanW8DXhAB1t2jo/dxrlpcbx843Sump7CP7/RenGlvIEm8JNoHtRq48HSXt1vrtULs6d14G1prhd/5P3N2u1eqQGuwwQuIi+JSIGIbGm1/EcislNEtorIk64L0X0mJIaTNCiQe9/Y0GJskp5qLoHHuyCBRwT58chFY1mfXcrraw72+v6VUv1HZ0rgLwMXOC8QkbnAfGCiMWYc8IfeD839gv19eP+u2UxKiuCe1zfwPx9v75XZcvJKjxIe6OuysUwum5LIzOGR/P7THRTpHJtKDVgdJnBjzDKg9RQxdwBPGGNqrW0KXBBbvxAd4s+im2dw7cwUnluWxS8/2NrjfbYeB7y3iQi/vXQ81XUN/O7/trvsOEop9+puHfgo4HQRyRCRpSJySnsbisitIpIpIpmFhYXdPJx7+fnY+O2lE7h4Yjwfbcrrcd1ybmnNCeOA97bU2FBuO2ME767P5du9xS49llLKPbqbwH2AQcBM4H7gTWmrzzdgjFlojEk3xqTHxMR083D9wxmjYjhSXc/uHs7Uk3uk2qUl8GZ3n5VKcmQgj76/mboG7eSj1EDT3QSeA7xrHFYDTUB074XVP80cFgVAxr7ul2grauopb2MiB1cI8LXz63nj2VtYxfPLs1x+PKVU3+puAn8fOAtAREYBfsCAnwMsOTKQ+PAAMrK6P2t8flnbEzm4ytwxsXxn/GD+98vdZBdX98kxlVJ9ozPNCF8DvgVGi0iOiNwEvAQMt5oWvg4sMF4wu4CIMGNYJBn7irs9mcLxccBdWwfu7LFL0vCxCY99sKXjjZVSHqPDdmzGmKvaWXVtL8fiEWYMj+L9DXnsLawiNTaky68/2Uw8rhIfHsidc1N56rOdHCypJjkyqM+OrZRyHe2J2UUzh/esHjyv9Ch2mxAb2nclcHCMLw6wNa+sT4+rlHIdTeBdNDQqiNhQ/27Xg+eV1jA4LAB7OxM5uMqYwaHYBB0jRakBRBN4F4kIM4ZHdbsevDfGAe+OAF87I2JC2KoJXKkBQxN4N8wYFsnh8loOdLFVhzGGrMIqkiL7PoEDjEsIY1u+JnClBgpN4N0wc3gk0PV68J2HKyiqrD1Wj97X0hLCyC+roaSqzi3HV0r1Lk3g3TAiJoToEL8u14Mv3+VoKn/6SPf0eRqXEA5oPbhSA4Um8G4QEaYPiyRjX9cS+LLdhYyMDSG+B5MZ90RafBigLVGUGig0gXfTqcOjyC09Sub+ziXxmvpGVu8r4fSR7hsPZlCwHwnhAVoPrtQAoQm8m747NYn48AAefX9Lp2aDX72vhNqGJk4f5d4hY9ISwrQlilIDhCbwbgr29+EXl4xjx6EKXv5mf4fbL99diJ/dxoxhka4P7iTSEsLJKqzkaJ3OXK+Up9ME3gPnj4vj7DGx/OmLXce6yLdn+e4i0ocOIsjPNbPwdFZafBhNBnYc0lK4Up5OE3gPiAi/nDeOJmP41Yftz9RTUF7DjkMVbq3/bjYuwXEjU+vBlfJ8msB7KDkyiHvOHsVnWw/z6ZZDbW6zfLd7mw86SxoUSGiAj9aDKzUAaALvBTefPoxxCWHc/9ZG9rQxW8/y3YVEBfsda8bnTiJCWnyYtgVXagDQBN4LfO02Fl6fjr+vjVv+nUlZdf2xdU1NhhV7ijhtZDS2Ph7Aqj3jEsLZcaicxh7O7amUci9N4L0kMSKQZ6+dRs6Rau5+bR0NjU1sPFjKT9/eSFFlHaelur/6pFlaQhg19U3sK+rZ3J5KKfdyb5OIASZ9aCSPXzqBB97ZxOzf/5fD5bUE+tq5ZkYKl0xKcHd4xzTfyNyaV05qbKibo1FKdZcm8F72/VOSOVBSxfLdRdw9N5X5UxIJC/B1d1gtjIgJQQSyCqvcHYpSqgc6TOAi8hJwMVBgjBnfat1PgaeAGGPMgJ/UuLPuP38M95/v7ija5+djIzLIj4KKWneHopTqgc7Ugb8MXNB6oYgkA+cC2b0ck+oDMaH+FJTXuDsMpVQPdJjAjTHLgLZGbHoaeADQpgweKC4sQEvgSnm4brVCEZF5QK4xZmMntr1VRDJFJLOwsLA7h1MuEBvqT0GFlsCV8mRdTuAiEgQ8AjzWme2NMQuNMenGmPSYGPd3JVcOsWH+FFXWaVtwpTxYd0rgI4BhwEYR2Q8kAetEZHBvBqZcKzY0gMYmQ3GVVqMo5am63IzQGLMZiG1+biXxdG2F4lniwvwBKCivJTY0wM3RKKW6o8MSuIi8BnwLjBaRHBG5yfVhKVeLsZJ2od7IVMpjdVgCN8Zc1cH6ob0WjeozsaFWCVxvZCrlsXQsFC8VE3q8CkUp5Zk0gXupAF874YG+HNYSuFIeSxO4F4sL89cSuFIeTBO4F4sN1d6YSnkyTeBeLDbUv1+2QjHGsPZACR9tynN3KEr1azqcrBeLCXN0pzfGIOL+2YIqaup5f30uizOy2XGoAoDxCeEMjQ52c2RK9U9aAvdicaEB1DcajjhNAecuxhiueSGDny/Zio9d+Ol5owBYvkf7hynVHk3gXiw2rP+0Bd+WX86mnDIeuXAsH/3odO6am0rSoECW79IB0JRqjyZwL9bchb4/tET5YEMePjbh8mlJAIgIp4+M4du9xdQ3Nrk5OqX6J03gXux4b0z3JvCmJsMHG/OYMyqGQcF+x5afMTKaitoGNhwsdWN0SvVfmsC9WHMVymE3z8yzZn8J+WU1zJ+S2GL5rBHR2AStRlGqHZrAvViQnw+h/j5ub0r4/oY8gvzsnDM2tsXy8CBfJiVHsGy33shUqi2awL1cc1NCd6lraOLjzfmclxZHkN+JrVpPHxnDppxSSqvr3BCdUv2bJnAvFxvq3u70y3YVUna0nvmTE9tcP2dUNE0Gvtlb3MeRKdX/aQL3cu7uTr9kYx6Dgnw5bWR0m+snJUUQ6u/D8t1aD65Ua5rAvVxcmD+Hyx29MftaZW0DX2w7xEUT4/G1t/1R9LHbmJUaxbJdRW6JUan+TBO4l4sNDaC2oYnymoY21x+ta3TJxMfGGB56dzO1DU1cMS35pNuePjKG3NKj7Cuq6vU4lPJkmsC9XHNTwsJ2bmRe8tcV3PSvNTT0cmeav3+9lw835nH/+aOZlBxx0m3PGBmDCNz48hqeW7qX4kr3dzxSqj/Qway8nPPMPKmxoS3WFVTUsKegkj0FlfzPJzv4+cVpvXLML7Yd5g+f72TepATumDOiw+1TooJ47tppvLB8H//zyQ7++PkuxieGYbc5BuAK8LVz4YR45k1KINhfP9LKe3RmUuOXRKRARLY4LXtKRHaIyCYReU9ETl6EUv1WXJjVnb6NG5mbc8oAmD40khdX7OPNzIM9Pt6uwxX85PX1TEgM58nLJ3Z6FMTzxg3mzdtP5fN7z+DqGSkE+NrxtdvwtdvIKz3KQ+9uZubvvuSxJVvILzva4ziV8gSdKa68DPwV+LfTsi+Ah4wxDSLye+Ah4MHeD0+5WnN3+rZ6Y27KKcMm8MIN6dy5aB2PvreFETHBTBsS2a1jGWN49L0tBPrZWXhdOgG+9i7vY1RcKL+cN+6E/a7LPsLiVdm8vuYgK3YX8d5dswkP9O1WnEp5ig5L4MaYZUBJq2WfG2Oa73qtApJcEJvqAyH+PgT62tssgW/KKSU1NoSwAF/+evUU4iMCuH3ROsq6OfzsqqwSVu8v4UdnjWRweEBPQz9GRJg2JJI/XZlNl+UAABIJSURBVDmZRTfNILukmh+9tt4lN1+V6k964ybmD4FP2lspIreKSKaIZBYWalve/kZEiA3zPyGBG2PYlFPGxCRH7VhEkB9/u3oqxZW1/P6zHd061v9+uZvYUH+uPOXkrU56YvqwSH5z6XiW7SrkiU+2A47BslbuKeLXH27rF0PnKtVbenTHR0QeARqAxe1tY4xZCCwESE9P1yJRPxQXGkBBqyqUvLIaiqvqmJQUfmzZ+MRwbpw9jJdW7uPyaUlMTRnU6WOs3lfCt1nF/PzitG5VnXTFVdNT2JFfzvPL91FZ28CqrJJjTRCD/Oz89PzRLj2+Un2l2yVwEVkAXAxcY7SHhUeLjwggq6iKJqcqh03WEK4Tklren7733FHEhQbwyHtbutS08C//3U10iB9XT0/pnaA78OjFacxOjeK11QeJDvHjz1dOZnZqFEs25mqHIDVgdCuBi8gFOG5azjPGVPduSKqvnT02jsKKWlbvP36rY1NuGb52YWx8y6aFIf4+/OKSNLbnl/PyN/s7tf+1B46wfHcRt54xnEA/15a+m/nabby44BSW3T+Xt26fxaVTEvnulCQOlhxlvY4vrgaIDqtQROQ14EwgWkRygF/gaHXiD3xhNQNbZYy53YVxKhc6Z2wsQX52lmzIY+bwKMBxA3P04FD8fU5MuBeMH8zc0TE8/cUuiqvqaG4IeNrIaGaNaDmmiTGGZ77cTWSwH9fMGOLqU2khwNdOSlTQsefnjYvD/z0bS9bndqn6R6n+qjOtUK4yxsQbY3yNMUnGmBeNManGmGRjzGTrR5O3Bwvy8+G8tDg+3pxPXUMTTU0tb2C2JiL8ev54BgX78cLyLJ5fnsWzS/ey4KXVZO5v0WCJl7/Zz7JdhdwxZ4TbO9mEBvhyztg4PtqU3+s9S5VyB+1KrwCYPzmRsqP1LNtVyIGSaipqGpiYGN7u9smRQax48Cx2P34hux+/kHU/P5fEiEBuX7SW3FJHR5rluwv5zUfbODctjptOG9ZXp3JS8yYnUFxVx0odnlYNAJrAFeCo/hgU5MuSjXlsynHUEbdXAm9LRJAfLyxIp6a+iVv+lcm2vHLuWryOkbGhPH3lZGy2zvW4dLUzR8cQGuDDkg257g5FqR7TBK4Ax02/iybG88W2Q6zKKsbfx8aouJAu7SM1NpS/XDWF7YfKmffXFdhtwgsL0gnpR+OT+PvYuXB8PJ9tOURNfSMA2cXVfL2zwM2RKdV1msDVMZdOTqSmvom31+YwLiEMn3bG6D6ZuWNieeTCsfj52Pj7NdNIjgzq+EV9bP7kBKrqGnnqs51c/9JqznjqK2745xq255e7OzSlukQTuDpmasogEiMCqW80Xao+ae3m04ez8RfnceqIqF6MrvfMGB5FXJg/L67Yx65DFdxx5ghE4POth90dmlJd0n++2yq3s9mEeZMT+MfXe5mY1P4NzM5ob4ad/sBuE567Lp3iylrmjIrBx24jI6uYL7Yf4p5zRro7PKU6rf/+lSm3uHp6CqelRnPGqBh3h+JSk5MjOHts3LFqonPTBrMlt1yHolUeRRO4aiE5MohFN88gOsTf3aH0qXPT4gD4zzatRlGeQxO4UsCImGCGRQfzuSZw5UE0gSuFo3fpuWlxrMoqpryme+OdK9XXNIErZTk3LY76RsPSnd49bn1VbQPVdQ0db6jcThO4UpapKYOICvbjCy+uRlmXfYS5f/ias/+49NicqKr/0gSulMVuE84aE8tXOwuo98LBrt7KPMgPnluFn48NAS5/9hsdcqCf03bgSjk5Ny2Ot9bmsCqrmNNH9rwpZVOT4dusYsIDfRl/ksHB+sKyXYVkFVa2uW7HoQpeX3OQWSOi+NvVU2k0hjsXreOe1zewLb+cn10wBmvoaNWPaAJXysnpI2MYFOTLvW9s4Nlrp5E+NLJTrzPGUF3XSPNcP5U1Dby/IZdXM7LJLqkmPNCX/943hyg3NM9saGzidx/v4KWV+0663Y2zh/LIhWOPtY1fdPMMfvnhVp5bmkWovw93n6WdnPob6cvppdLT001mZmafHU+p7thTUMHN/8okt/Qov54/nqtOMg1cZW0DS6xEvTXvxLFUZgyL5ILxg/ndx9uZNymRP35/ksviLiiv4UevrWdvYRXfm5bI1dNTCA/05e5X17NiTxE3zh7K3XNTsbVRkvaxC6EBvicsN8bw/97cyHvrc1l43TTOGzfYZfGr9onIWmNM+gnLNYErdaKy6np+9Pp6lu0q5LqZQ3jskrQWwwPUNTTxx893smjVAarqGhkzOJSLJsQfm7DZZhPmjIomNdYxJd1Tn+3gb1/t5fVbZx6b9agr/rlyH6uyjo9hHhrgy2VTEpk1IgoRYcPBUm57JZOKmgZmDo9i6a5CGpsMEUG+VNc28tvLxvP99ORuXYua+kauXLiKPYcreOfOWYwZHNat/aju0wSuVBc1Nhme/HQHzy3LYvqwSP5xzVSiQvwpqqzljkVrWbP/CJdOTuD6WUOZkhxx0jrio3WNnPv0UgJ87Xz849Px8+l8+4FXM7J5+L3NDIkKItD6B5FXepTymgaGRwczZ3QMizOyiQ315/nr0xkbH8bh8hreWHOQ9dlHuPuskUwb0rMp5A6X13DJX1bg52NjyV2z3VIV5M26ncBF5CUcs88XGGPGW8sigTeAocB+4PvGmCMdBaEJXHmi99fn8uA7m4gO8ef+80fz5Kc7KKmu48nLJzFvUkKn9/PVjgJufHkN958/mrvmpnbqNRlZxVzzQgazU6N56YZTsFsTY9TUN/LJlnwWr8om88ARTh0exd+umUpksF+3zrEzNhws5crnviUhIpDnr5927NuFcr2eJPAzgErg304J/EmgxBjzhIj8DBhkjHmwoyA0gStPtTmnjFtfySS/rIaE8AAWXp/erVYlt7+yls+3HTo2yYXdJvz0/NFtTvicc6SaeX9dSUSQL+/dOZvwwBPrqAGKK2sZFOTXJ7Merdlfwh2L1lJT38Sfr5zMOdYYMsq1elSFIiJDgY+cEvhO4ExjTL6IxANfG2NGd7QfTeDKkxVW1PLGmmx+MD2l24N9FVfW8uKKfVTXOWYDWn+wlB355fzn/81pMflFdV0D3/37N+SWHmXJXbMZHtO12ZFcKa/0KLe9spYteWXcdsYIbjptGDGhWqXiSr2dwEuNMRFO648YY9qsZBORW4FbAVJSUqYdOHCgWyeg1ECUX3aUc/64lBnDo3hxQToiQlOT4c7F6/h82yH+eeN05vTDoX1r6ht55L0tvLMuBx+bcP74wVw3c0i3btCqjrWXwF3eE9MYs9AYk26MSY+J6X8fRKXcKT48kHvPHcV/dxTwmTUj0DNf7ubTrYd4+MKx/TJ5AwT42vnj9yfx5X1zuGHWUFbsLuIHC1dpz80+1t0EftiqOsH6rTPCKtVNN8waytj4MH714VbeyjzIM1/u5vJpSdx02jB3h9ahETEhPHpxGhkPn82k5Ah+89E2yqpbjub4+dZD3Ll4rY7y6ALdTeAfAAusxwuAJb0TjlLex8du4/HLxnOovIb7397E1JQIHr9svEd1XQ/wtfP4peMpqarjqc93HFu+OaeMH722no83H+Inr2+gsanvmi17gw4TuIi8BnwLjBaRHBG5CXgCOFdEdgPnWs+VUt00NWUQN582jGHRwTx73TT8fezuDqnLxieGc8OsYSzOyGZ99hEKKmq45d+ZRIf4c59VTfTkZzs63pHqNO3Io1Q/0tRk+qQ5oKtU1jZwzh+XEhnsR4Cvje35Fbx9x6mMSwjnkfc2szgjm6evnMRlU5LcHapHae8mpg5mpVQ/4snJGyDE34fHLknjzsXrAPjHNVMZl+BoL//LeePYU1DJg+9s5sON+bR1pqEBPtx33ugWTSq7q7S6jj9+vouzx8Zy5ujYHu+vP9ISuFKqVxlj+N3H20mJDOK6U4e2WFdSVcf9b23kcEVNm6/dX1SNn4+Nv18ztdNNErfnl+PnY2OEU1v5nYcquOXfmWSXVCMCD5w/htvnDO/yfQVjDJtzy0iJDCIiyHW9XDuiY6Eopfq9rMJKbvl3JgeKq3nskjSumzmkzaRbVdvABxvzWJxxgC25jlEgpw+L5JoZKfjYbDzw9kaC/H3485WTeW11Nh9tyueSSQk8+b2JBPp17v5CTX0jD7+7mXfX5+LvY+PiiQlcMzOlw3FvXEETuFLKI1TU1POT1zfw5Y4CZo2I4rqZQzgnLQ5fu40dh8pZvCqb99bnUlnbwOi4UK6ZmcLRukZeXZ3NgeJqACYlR/DctdMYHB6AMYZ/LN3LU5/tdHwrmDmEy6clERHkR3FlLW+tzeGdtTlEBvtx9YwULhg/mCNV9dz2SiYbc8q448wRlB+t5/31uVTVNTI7NYr//cGUPh3QSxO4UspjNDUZXlq5j3+u3E9u6VFiQv1JjAhkw8FS/HxsXDQhnmtnpjA1ZdCx0nBTk2Hl3iL2F1VxRXrysaF9my3bVcgzX+5m7YEj+PnYmJYyiMwDJdQ3GtKHDKKgopbskmoig/2wiWMEyaevnHxsDPTK2gbeXHOQ33+6g+gQfxZeP+1Y/b6zo3WNfLQpD2Pg8mlJvXJfQxO4UsrjNDYZvt5ZwOKMbPLLavje1ES+NzWJQT0YdXF7fjmvZmSzck8Rc0bHcPX0FEbGhdLUZFixp4hFqw5wpLqOxy+bwKi4E0dc3JRTym2vrOVIdR2/njee0YMd29Q2NPHx5nzeWZdDRU0DAOeMjePpKye1OVlGV2gCV0qpXlJQUcMdi9ax9kDLUbR97cJ3xsdzzYwUtueX85v/286w6GCevz6dYdHB3T6eJnCllOpFdQ1NZOwrpr6xCQBBmJAU3mKkym/2FnHX4nU0NhleWHAK04d1bo7V1rQduFJK9SI/Hxunjzz5YGOzRkTzwd2n8fB7m0kcFNjrMWgCV0opF0qODOKVm2a4ZN8uH05WKaWUa2gCV0opD6UJXCmlPJQmcKWU8lCawJVSykNpAldKKQ+lCVwppTyUJnCllPJQfdqVXkQKgQPdfHk0UNSL4Xg6vR7H6bVoSa9HSwPhegwxxpzQ7bNPE3hPiEhmW2MBeCu9HsfptWhJr0dLA/l6aBWKUkp5KE3gSinloTwpgS90dwD9jF6P4/RatKTXo6UBez08pg5cKaVUS55UAldKKeVEE7hSSnkoj0jgInKBiOwUkT0i8jN3x9OXRCRZRL4Ske0islVE7rGWR4rIFyKy2/o9yN2x9iURsYvIehH5yHrulddDRCJE5G0R2WF9Rk711msBICL3Wn8nW0TkNREJGMjXo98ncBGxA38DvgOkAVeJSJp7o+pTDcB9xpixwEzgLuv8fwZ8aYwZCXxpPfcm9wDbnZ576/V4BvjUGDMGmITjmnjltRCRRODHQLoxZjxgB37AAL4e/T6BA9OBPcaYLGNMHfA6MN/NMfUZY0y+MWad9bgCxx9oIo5r8C9rs38Bl7onwr4nIknARcALTou97nqISBhwBvAigDGmzhhTihdeCyc+QKCI+ABBQB4D+Hp4QgJPBA46Pc+xlnkdERkKTAEygDhjTD44kjwQ677I+tyfgQeAJqdl3ng9hgOFwD+t6qQXRCQY77wWGGNygT8A2UA+UGaM+ZwBfD08IYFLG8u8ru2jiIQA7wA/McaUuzsedxGRi4ECY8xad8fSD/gAU4F/GGOmAFUMoOqBrrLqtucDw4AEIFhErnVvVK7lCQk8B0h2ep6E42uR1xARXxzJe7Ex5l1r8WERibfWxwMF7oqvj80G5onIfhzVaWeJyCK883rkADnGmAzr+ds4Ero3XguAc4B9xphCY0w98C4wiwF8PTwhga8BRorIMBHxw3FT4gM3x9RnRERw1HFuN8b8yWnVB8AC6/ECYElfx+YOxpiHjDFJxpihOD4L/zXGXIsXXg9jzCHgoIiMthadDWzDC6+FJRuYKSJB1t/N2TjuGQ3Y6+ERPTFF5EIc9Z524CVjzONuDqnPiMhpwHJgM8frfB/GUQ/+JpCC44N7hTGmxC1BuomInAn81BhzsYhE4YXXQ0Qm47iZ6wdkATfiKJh53bUAEJFfAVfiaL21HrgZCGGAXg+PSOBKKaVO5AlVKEoppdqgCVwppTyUJnCllPJQmsCVUspDaQJXSikPpQlcKaU8lCZwpZTyUP8fcn1iZsoLdakAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And see the results\n",
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "plt.plot(smooth(episode_durations, 10))\n",
    "plt.title('Episode durations per episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dqn_autograde.py file into codegrade.**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
